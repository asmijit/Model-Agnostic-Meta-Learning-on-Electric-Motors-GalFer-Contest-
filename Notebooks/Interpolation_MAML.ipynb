{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea979ae-1df4-40d1-804a-4e3fbad45b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_alpha</th>\n",
       "      <th>h_c</th>\n",
       "      <th>r</th>\n",
       "      <th>w_t</th>\n",
       "      <th>l_t</th>\n",
       "      <th>w_o</th>\n",
       "      <th>dxIB</th>\n",
       "      <th>gamma</th>\n",
       "      <th>d_alpha_deg</th>\n",
       "      <th>hc_mm</th>\n",
       "      <th>w_o_mm</th>\n",
       "      <th>T</th>\n",
       "      <th>TR</th>\n",
       "      <th>m_Cu</th>\n",
       "      <th>m_mag</th>\n",
       "      <th>cos_phi</th>\n",
       "      <th>VM</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683555</td>\n",
       "      <td>0.652960</td>\n",
       "      <td>76.388367</td>\n",
       "      <td>4.876673</td>\n",
       "      <td>15.980657</td>\n",
       "      <td>0.252080</td>\n",
       "      <td>3.958199</td>\n",
       "      <td>47.979187</td>\n",
       "      <td>20.5065</td>\n",
       "      <td>13.333585</td>\n",
       "      <td>2.261068</td>\n",
       "      <td>322.034019</td>\n",
       "      <td>46.460214</td>\n",
       "      <td>3.496543</td>\n",
       "      <td>1.637520</td>\n",
       "      <td>0.600832</td>\n",
       "      <td>449.306137</td>\n",
       "      <td>161.493855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.839970</td>\n",
       "      <td>0.316877</td>\n",
       "      <td>66.528144</td>\n",
       "      <td>5.477306</td>\n",
       "      <td>21.227689</td>\n",
       "      <td>0.125430</td>\n",
       "      <td>-0.907076</td>\n",
       "      <td>38.605065</td>\n",
       "      <td>25.1991</td>\n",
       "      <td>2.806290</td>\n",
       "      <td>0.981157</td>\n",
       "      <td>319.186347</td>\n",
       "      <td>26.164250</td>\n",
       "      <td>3.422449</td>\n",
       "      <td>0.827423</td>\n",
       "      <td>0.460323</td>\n",
       "      <td>462.037013</td>\n",
       "      <td>163.451036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.430152</td>\n",
       "      <td>70.092293</td>\n",
       "      <td>5.902179</td>\n",
       "      <td>17.496837</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>2.421620</td>\n",
       "      <td>32.137510</td>\n",
       "      <td>22.4898</td>\n",
       "      <td>6.407833</td>\n",
       "      <td>1.638763</td>\n",
       "      <td>365.429403</td>\n",
       "      <td>18.859974</td>\n",
       "      <td>2.550712</td>\n",
       "      <td>1.383712</td>\n",
       "      <td>0.601910</td>\n",
       "      <td>376.266906</td>\n",
       "      <td>175.349022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.779947</td>\n",
       "      <td>0.588121</td>\n",
       "      <td>64.097056</td>\n",
       "      <td>3.983202</td>\n",
       "      <td>19.711625</td>\n",
       "      <td>0.325027</td>\n",
       "      <td>-2.712820</td>\n",
       "      <td>52.763373</td>\n",
       "      <td>23.3985</td>\n",
       "      <td>6.980233</td>\n",
       "      <td>2.450559</td>\n",
       "      <td>388.445550</td>\n",
       "      <td>43.657137</td>\n",
       "      <td>4.056152</td>\n",
       "      <td>1.559041</td>\n",
       "      <td>0.687662</td>\n",
       "      <td>362.526070</td>\n",
       "      <td>161.401494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671491</td>\n",
       "      <td>0.359245</td>\n",
       "      <td>61.665401</td>\n",
       "      <td>6.232320</td>\n",
       "      <td>21.751120</td>\n",
       "      <td>0.294789</td>\n",
       "      <td>1.796389</td>\n",
       "      <td>57.935310</td>\n",
       "      <td>20.1447</td>\n",
       "      <td>6.039823</td>\n",
       "      <td>2.139157</td>\n",
       "      <td>316.498500</td>\n",
       "      <td>43.529585</td>\n",
       "      <td>2.219354</td>\n",
       "      <td>0.930888</td>\n",
       "      <td>0.656835</td>\n",
       "      <td>265.543625</td>\n",
       "      <td>183.992897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>0.795862</td>\n",
       "      <td>0.389833</td>\n",
       "      <td>61.630750</td>\n",
       "      <td>4.726709</td>\n",
       "      <td>21.116910</td>\n",
       "      <td>0.391967</td>\n",
       "      <td>-0.356609</td>\n",
       "      <td>38.449031</td>\n",
       "      <td>23.8758</td>\n",
       "      <td>4.098186</td>\n",
       "      <td>2.842765</td>\n",
       "      <td>330.704739</td>\n",
       "      <td>52.500055</td>\n",
       "      <td>3.502802</td>\n",
       "      <td>0.961655</td>\n",
       "      <td>0.525826</td>\n",
       "      <td>364.070648</td>\n",
       "      <td>166.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>0.655776</td>\n",
       "      <td>0.557722</td>\n",
       "      <td>64.138009</td>\n",
       "      <td>5.569651</td>\n",
       "      <td>19.406519</td>\n",
       "      <td>0.286441</td>\n",
       "      <td>5.134185</td>\n",
       "      <td>41.751688</td>\n",
       "      <td>19.6734</td>\n",
       "      <td>10.213618</td>\n",
       "      <td>2.160973</td>\n",
       "      <td>258.916385</td>\n",
       "      <td>29.670157</td>\n",
       "      <td>2.627395</td>\n",
       "      <td>1.026045</td>\n",
       "      <td>0.541985</td>\n",
       "      <td>309.458343</td>\n",
       "      <td>176.949769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>0.824020</td>\n",
       "      <td>0.424761</td>\n",
       "      <td>70.060731</td>\n",
       "      <td>4.900752</td>\n",
       "      <td>17.915470</td>\n",
       "      <td>0.113394</td>\n",
       "      <td>0.268017</td>\n",
       "      <td>51.125728</td>\n",
       "      <td>24.7206</td>\n",
       "      <td>4.420288</td>\n",
       "      <td>0.933583</td>\n",
       "      <td>390.552354</td>\n",
       "      <td>52.783734</td>\n",
       "      <td>3.411842</td>\n",
       "      <td>1.220441</td>\n",
       "      <td>0.620082</td>\n",
       "      <td>445.986053</td>\n",
       "      <td>164.116517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>0.714139</td>\n",
       "      <td>0.334924</td>\n",
       "      <td>66.488205</td>\n",
       "      <td>3.919224</td>\n",
       "      <td>21.625679</td>\n",
       "      <td>0.187517</td>\n",
       "      <td>1.236156</td>\n",
       "      <td>58.018170</td>\n",
       "      <td>21.4242</td>\n",
       "      <td>5.356057</td>\n",
       "      <td>1.465975</td>\n",
       "      <td>360.733017</td>\n",
       "      <td>44.419942</td>\n",
       "      <td>5.033340</td>\n",
       "      <td>1.066646</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>321.930873</td>\n",
       "      <td>151.601869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>0.759187</td>\n",
       "      <td>0.695997</td>\n",
       "      <td>76.420941</td>\n",
       "      <td>5.769777</td>\n",
       "      <td>15.696193</td>\n",
       "      <td>0.361159</td>\n",
       "      <td>-3.897957</td>\n",
       "      <td>37.392224</td>\n",
       "      <td>22.7757</td>\n",
       "      <td>10.966983</td>\n",
       "      <td>3.240842</td>\n",
       "      <td>379.812048</td>\n",
       "      <td>42.086200</td>\n",
       "      <td>2.812695</td>\n",
       "      <td>2.333271</td>\n",
       "      <td>0.660957</td>\n",
       "      <td>450.984029</td>\n",
       "      <td>169.259025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       d_alpha       h_c          r       w_t        l_t       w_o      dxIB  \\\n",
       "0     0.683555  0.652960  76.388367  4.876673  15.980657  0.252080  3.958199   \n",
       "1     0.839970  0.316877  66.528144  5.477306  21.227689  0.125430 -0.907076   \n",
       "2     0.749656  0.430152  70.092293  5.902179  17.496837  0.198953  2.421620   \n",
       "3     0.779947  0.588121  64.097056  3.983202  19.711625  0.325027 -2.712820   \n",
       "4     0.671491  0.359245  61.665401  6.232320  21.751120  0.294789  1.796389   \n",
       "...        ...       ...        ...       ...        ...       ...       ...   \n",
       "4091  0.795862  0.389833  61.630750  4.726709  21.116910  0.391967 -0.356609   \n",
       "4092  0.655776  0.557722  64.138009  5.569651  19.406519  0.286441  5.134185   \n",
       "4093  0.824020  0.424761  70.060731  4.900752  17.915470  0.113394  0.268017   \n",
       "4094  0.714139  0.334924  66.488205  3.919224  21.625679  0.187517  1.236156   \n",
       "4095  0.759187  0.695997  76.420941  5.769777  15.696193  0.361159 -3.897957   \n",
       "\n",
       "          gamma  d_alpha_deg      hc_mm    w_o_mm           T         TR  \\\n",
       "0     47.979187      20.5065  13.333585  2.261068  322.034019  46.460214   \n",
       "1     38.605065      25.1991   2.806290  0.981157  319.186347  26.164250   \n",
       "2     32.137510      22.4898   6.407833  1.638763  365.429403  18.859974   \n",
       "3     52.763373      23.3985   6.980233  2.450559  388.445550  43.657137   \n",
       "4     57.935310      20.1447   6.039823  2.139157  316.498500  43.529585   \n",
       "...         ...          ...        ...       ...         ...        ...   \n",
       "4091  38.449031      23.8758   4.098186  2.842765  330.704739  52.500055   \n",
       "4092  41.751688      19.6734  10.213618  2.160973  258.916385  29.670157   \n",
       "4093  51.125728      24.7206   4.420288  0.933583  390.552354  52.783734   \n",
       "4094  58.018170      21.4242   5.356057  1.465975  360.733017  44.419942   \n",
       "4095  37.392224      22.7757  10.966983  3.240842  379.812048  42.086200   \n",
       "\n",
       "          m_Cu     m_mag   cos_phi          VM        Temp  \n",
       "0     3.496543  1.637520  0.600832  449.306137  161.493855  \n",
       "1     3.422449  0.827423  0.460323  462.037013  163.451036  \n",
       "2     2.550712  1.383712  0.601910  376.266906  175.349022  \n",
       "3     4.056152  1.559041  0.687662  362.526070  161.401494  \n",
       "4     2.219354  0.930888  0.656835  265.543625  183.992897  \n",
       "...        ...       ...       ...         ...         ...  \n",
       "4091  3.502802  0.961655  0.525826  364.070648  166.587400  \n",
       "4092  2.627395  1.026045  0.541985  309.458343  176.949769  \n",
       "4093  3.411842  1.220441  0.620082  445.986053  164.116517  \n",
       "4094  5.033340  1.066646  0.585498  321.930873  151.601869  \n",
       "4095  2.812695  2.333271  0.660957  450.984029  169.259025  \n",
       "\n",
       "[4096 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mot_a=pd.read_csv('Mot_A_rel_2.csv')\n",
    "mot_b=pd.read_csv('Mot_B_rel_1.csv')\n",
    "mot_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7237ac-edcd-4c95-82cc-1068e4b232f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_alpha</th>\n",
       "      <th>h_c</th>\n",
       "      <th>r</th>\n",
       "      <th>w_t</th>\n",
       "      <th>l_t</th>\n",
       "      <th>w_o</th>\n",
       "      <th>dxIB</th>\n",
       "      <th>gamma</th>\n",
       "      <th>d_alpha_deg</th>\n",
       "      <th>hc_mm</th>\n",
       "      <th>w_o_mm</th>\n",
       "      <th>T</th>\n",
       "      <th>TR</th>\n",
       "      <th>m_Cu</th>\n",
       "      <th>m_mag</th>\n",
       "      <th>cos_phi</th>\n",
       "      <th>VM</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.475498</td>\n",
       "      <td>0.652960</td>\n",
       "      <td>88.388367</td>\n",
       "      <td>6.722678</td>\n",
       "      <td>15.138103</td>\n",
       "      <td>0.252080</td>\n",
       "      <td>5.549839</td>\n",
       "      <td>58.972249</td>\n",
       "      <td>10.698750</td>\n",
       "      <td>17.333507</td>\n",
       "      <td>2.940657</td>\n",
       "      <td>107.431396</td>\n",
       "      <td>20.354274</td>\n",
       "      <td>3.096863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338882</td>\n",
       "      <td>273.035025</td>\n",
       "      <td>190.758002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.827432</td>\n",
       "      <td>0.316877</td>\n",
       "      <td>78.528144</td>\n",
       "      <td>7.683689</td>\n",
       "      <td>31.928606</td>\n",
       "      <td>0.125430</td>\n",
       "      <td>-0.288491</td>\n",
       "      <td>46.473420</td>\n",
       "      <td>18.617175</td>\n",
       "      <td>2.579520</td>\n",
       "      <td>1.301319</td>\n",
       "      <td>163.073859</td>\n",
       "      <td>23.559694</td>\n",
       "      <td>6.629627</td>\n",
       "      <td>0.333044</td>\n",
       "      <td>0.336489</td>\n",
       "      <td>597.286490</td>\n",
       "      <td>148.540980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.624227</td>\n",
       "      <td>0.430152</td>\n",
       "      <td>82.092293</td>\n",
       "      <td>8.363486</td>\n",
       "      <td>19.989879</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>3.705944</td>\n",
       "      <td>37.850013</td>\n",
       "      <td>14.045175</td>\n",
       "      <td>8.369918</td>\n",
       "      <td>2.156899</td>\n",
       "      <td>169.927802</td>\n",
       "      <td>28.158900</td>\n",
       "      <td>2.785970</td>\n",
       "      <td>0.464450</td>\n",
       "      <td>0.481598</td>\n",
       "      <td>329.469220</td>\n",
       "      <td>182.243621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.692381</td>\n",
       "      <td>0.588121</td>\n",
       "      <td>76.097056</td>\n",
       "      <td>5.293124</td>\n",
       "      <td>27.077199</td>\n",
       "      <td>0.325027</td>\n",
       "      <td>-2.455384</td>\n",
       "      <td>65.351164</td>\n",
       "      <td>15.578550</td>\n",
       "      <td>8.642762</td>\n",
       "      <td>3.268711</td>\n",
       "      <td>166.232109</td>\n",
       "      <td>32.301448</td>\n",
       "      <td>7.090539</td>\n",
       "      <td>0.628584</td>\n",
       "      <td>0.469956</td>\n",
       "      <td>440.461090</td>\n",
       "      <td>161.432571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.448356</td>\n",
       "      <td>0.359245</td>\n",
       "      <td>73.665401</td>\n",
       "      <td>8.891712</td>\n",
       "      <td>33.603582</td>\n",
       "      <td>0.294789</td>\n",
       "      <td>2.955666</td>\n",
       "      <td>72.247080</td>\n",
       "      <td>10.088100</td>\n",
       "      <td>8.872504</td>\n",
       "      <td>2.870764</td>\n",
       "      <td>45.721275</td>\n",
       "      <td>13.713770</td>\n",
       "      <td>4.474470</td>\n",
       "      <td>0.086973</td>\n",
       "      <td>0.109926</td>\n",
       "      <td>97.144168</td>\n",
       "      <td>151.689880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>0.728189</td>\n",
       "      <td>0.389833</td>\n",
       "      <td>73.630750</td>\n",
       "      <td>6.482734</td>\n",
       "      <td>31.574111</td>\n",
       "      <td>0.391967</td>\n",
       "      <td>0.372069</td>\n",
       "      <td>46.265375</td>\n",
       "      <td>16.384275</td>\n",
       "      <td>4.858959</td>\n",
       "      <td>3.815355</td>\n",
       "      <td>187.225587</td>\n",
       "      <td>20.115794</td>\n",
       "      <td>7.124024</td>\n",
       "      <td>0.433161</td>\n",
       "      <td>0.421116</td>\n",
       "      <td>390.664324</td>\n",
       "      <td>153.238568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>0.412996</td>\n",
       "      <td>0.557722</td>\n",
       "      <td>76.138009</td>\n",
       "      <td>7.831442</td>\n",
       "      <td>26.100860</td>\n",
       "      <td>0.286441</td>\n",
       "      <td>6.961023</td>\n",
       "      <td>50.668917</td>\n",
       "      <td>9.292500</td>\n",
       "      <td>12.721049</td>\n",
       "      <td>2.882159</td>\n",
       "      <td>90.162651</td>\n",
       "      <td>20.057245</td>\n",
       "      <td>4.070937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224451</td>\n",
       "      <td>71.719715</td>\n",
       "      <td>166.776948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>0.791545</td>\n",
       "      <td>0.424761</td>\n",
       "      <td>82.060731</td>\n",
       "      <td>6.761203</td>\n",
       "      <td>21.329504</td>\n",
       "      <td>0.113394</td>\n",
       "      <td>1.121620</td>\n",
       "      <td>63.167637</td>\n",
       "      <td>17.809650</td>\n",
       "      <td>4.513207</td>\n",
       "      <td>1.228839</td>\n",
       "      <td>206.950266</td>\n",
       "      <td>21.267395</td>\n",
       "      <td>4.346549</td>\n",
       "      <td>0.537655</td>\n",
       "      <td>0.556379</td>\n",
       "      <td>602.523742</td>\n",
       "      <td>173.742174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>0.544312</td>\n",
       "      <td>0.334924</td>\n",
       "      <td>78.488205</td>\n",
       "      <td>5.190758</td>\n",
       "      <td>33.202173</td>\n",
       "      <td>0.187517</td>\n",
       "      <td>2.283387</td>\n",
       "      <td>72.357560</td>\n",
       "      <td>12.246975</td>\n",
       "      <td>7.445202</td>\n",
       "      <td>1.944515</td>\n",
       "      <td>49.078392</td>\n",
       "      <td>20.198410</td>\n",
       "      <td>10.718104</td>\n",
       "      <td>0.296825</td>\n",
       "      <td>0.114388</td>\n",
       "      <td>221.830783</td>\n",
       "      <td>144.123557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>0.645672</td>\n",
       "      <td>0.695997</td>\n",
       "      <td>88.420941</td>\n",
       "      <td>8.151644</td>\n",
       "      <td>14.227817</td>\n",
       "      <td>0.361159</td>\n",
       "      <td>-3.877549</td>\n",
       "      <td>44.856298</td>\n",
       "      <td>14.527575</td>\n",
       "      <td>13.871938</td>\n",
       "      <td>4.214674</td>\n",
       "      <td>173.633565</td>\n",
       "      <td>49.015104</td>\n",
       "      <td>2.157447</td>\n",
       "      <td>0.773399</td>\n",
       "      <td>0.606747</td>\n",
       "      <td>566.002951</td>\n",
       "      <td>198.465371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       d_alpha       h_c          r       w_t        l_t       w_o      dxIB  \\\n",
       "0     0.475498  0.652960  88.388367  6.722678  15.138103  0.252080  5.549839   \n",
       "1     0.827432  0.316877  78.528144  7.683689  31.928606  0.125430 -0.288491   \n",
       "2     0.624227  0.430152  82.092293  8.363486  19.989879  0.198953  3.705944   \n",
       "3     0.692381  0.588121  76.097056  5.293124  27.077199  0.325027 -2.455384   \n",
       "4     0.448356  0.359245  73.665401  8.891712  33.603582  0.294789  2.955666   \n",
       "...        ...       ...        ...       ...        ...       ...       ...   \n",
       "4091  0.728189  0.389833  73.630750  6.482734  31.574111  0.391967  0.372069   \n",
       "4092  0.412996  0.557722  76.138009  7.831442  26.100860  0.286441  6.961023   \n",
       "4093  0.791545  0.424761  82.060731  6.761203  21.329504  0.113394  1.121620   \n",
       "4094  0.544312  0.334924  78.488205  5.190758  33.202173  0.187517  2.283387   \n",
       "4095  0.645672  0.695997  88.420941  8.151644  14.227817  0.361159 -3.877549   \n",
       "\n",
       "          gamma  d_alpha_deg      hc_mm    w_o_mm           T         TR  \\\n",
       "0     58.972249    10.698750  17.333507  2.940657  107.431396  20.354274   \n",
       "1     46.473420    18.617175   2.579520  1.301319  163.073859  23.559694   \n",
       "2     37.850013    14.045175   8.369918  2.156899  169.927802  28.158900   \n",
       "3     65.351164    15.578550   8.642762  3.268711  166.232109  32.301448   \n",
       "4     72.247080    10.088100   8.872504  2.870764   45.721275  13.713770   \n",
       "...         ...          ...        ...       ...         ...        ...   \n",
       "4091  46.265375    16.384275   4.858959  3.815355  187.225587  20.115794   \n",
       "4092  50.668917     9.292500  12.721049  2.882159   90.162651  20.057245   \n",
       "4093  63.167637    17.809650   4.513207  1.228839  206.950266  21.267395   \n",
       "4094  72.357560    12.246975   7.445202  1.944515   49.078392  20.198410   \n",
       "4095  44.856298    14.527575  13.871938  4.214674  173.633565  49.015104   \n",
       "\n",
       "           m_Cu     m_mag   cos_phi          VM        Temp  \n",
       "0      3.096863  0.000000  0.338882  273.035025  190.758002  \n",
       "1      6.629627  0.333044  0.336489  597.286490  148.540980  \n",
       "2      2.785970  0.464450  0.481598  329.469220  182.243621  \n",
       "3      7.090539  0.628584  0.469956  440.461090  161.432571  \n",
       "4      4.474470  0.086973  0.109926   97.144168  151.689880  \n",
       "...         ...       ...       ...         ...         ...  \n",
       "4091   7.124024  0.433161  0.421116  390.664324  153.238568  \n",
       "4092   4.070937  0.000000  0.224451   71.719715  166.776948  \n",
       "4093   4.346549  0.537655  0.556379  602.523742  173.742174  \n",
       "4094  10.718104  0.296825  0.114388  221.830783  144.123557  \n",
       "4095   2.157447  0.773399  0.606747  566.002951  198.465371  \n",
       "\n",
       "[4096 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbd54e7-1384-4474-858a-80d80ab9fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Meta Loss: 1.0318819284439087\n",
      "Epoch 100, Meta Loss: 0.10082574188709259\n",
      "Epoch 200, Meta Loss: 0.07053263485431671\n",
      "Epoch 300, Meta Loss: 0.05477414280176163\n",
      "Epoch 400, Meta Loss: 0.044177211821079254\n",
      "Epoch 500, Meta Loss: 0.03613158315420151\n",
      "Epoch 600, Meta Loss: 0.02990984171628952\n",
      "Epoch 700, Meta Loss: 0.025143813341856003\n",
      "Epoch 800, Meta Loss: 0.02235828898847103\n",
      "Epoch 900, Meta Loss: 0.019736269488930702\n",
      "Final MSE on 20% test set: 90.40126037597656\n",
      "Final RMSE on 20% test set: 9.507957739492564\n",
      "Final MAE on 20% test set: 3.7741806507110596\n",
      "Final RÂ² Score on 20% test set: 0.9724400639533997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    mot_a = pd.read_csv(\"Mot_A_rel_2.csv\")\n",
    "    mot_b = pd.read_csv(\"Mot_B_rel_1.csv\")\n",
    "    \n",
    "    X_a, Y_a = mot_a.iloc[:, :11].values, mot_a.iloc[:, 11:].values\n",
    "    X_b, Y_b = mot_b.iloc[:, :11].values, mot_b.iloc[:, 11:].values\n",
    "    \n",
    "    X_combined = np.vstack([X_a, X_b])\n",
    "    Y_combined = np.vstack([Y_a, Y_b])\n",
    "    \n",
    "    scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "    X_combined = scaler_x.fit_transform(X_combined)\n",
    "    Y_combined = scaler_y.fit_transform(Y_combined)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_combined, Y_combined, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return (torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32)), \\\n",
    "           (torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.float32)), \\\n",
    "           scaler_x, scaler_y\n",
    "\n",
    "# Define MAML Model\n",
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self, input_size=11, output_size=7, hidden_size=128):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# MAML Meta-Learning Class\n",
    "class MAML:\n",
    "    def __init__(self, model, meta_lr=0.001, inner_lr=0.01, inner_steps=1, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    def inner_update(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = MAMLModel().to(self.device)\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(self.inner_steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)  # âœ… Ensure graph is retained\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "    def meta_train(self, X_train, y_train, epochs=1000):\n",
    "        X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model_copy = self.inner_update(X_train, y_train)\n",
    "            loss = self.loss_fn(self.model(X_train), y_train)  # âœ… Compute loss using meta-model\n",
    "            \n",
    "            self.meta_optimizer.zero_grad()\n",
    "            grads = torch.autograd.grad(loss, self.model.parameters(), create_graph=True, retain_graph=True)\n",
    "            \n",
    "            for param, grad in zip(self.model.parameters(), grads):\n",
    "                if grad is not None:\n",
    "                    param.grad = grad  # âœ… Correctly setting gradient\n",
    "            \n",
    "            self.meta_optimizer.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Meta Loss: {loss.item()}\")\n",
    "    \n",
    "    def fine_tune(self, X, y, steps=10):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = self.inner_update(X, y)\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "# Run MAML Training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "(X_train, y_train), (X_test, y_test), scaler_x, scaler_y = load_data()\n",
    "\n",
    "model = MAMLModel()\n",
    "maml = MAML(model, device=device)\n",
    "\n",
    "maml.meta_train(X_train, y_train)\n",
    "\n",
    "adapted_model = maml.fine_tune(X_test, y_test)\n",
    "\n",
    "y_pred = adapted_model(X_test.to(device)).detach().cpu().numpy()\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "y_test = scaler_y.inverse_transform(y_test.numpy())\n",
    "\n",
    "# Rigorous evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final MSE on 20% test set: {mse}\")\n",
    "print(f\"Final RMSE on 20% test set: {rmse}\")\n",
    "print(f\"Final MAE on 20% test set: {mae}\")\n",
    "print(f\"Final RÂ² Score on 20% test set: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076955c-0f33-47f4-9ac2-6af7efd8a67b",
   "metadata": {},
   "source": [
    "# Trained on A and finetuned on B. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d378559-35fb-46e4-bb05-d84a29b1d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Meta Loss: 0.5035821795463562\n",
      "Epoch 100, Meta Loss: 0.03363952413201332\n",
      "Epoch 200, Meta Loss: 0.016799943521618843\n",
      "Epoch 300, Meta Loss: 0.01089937798678875\n",
      "Epoch 400, Meta Loss: 0.008156727999448776\n",
      "Epoch 500, Meta Loss: 0.006705856882035732\n",
      "Epoch 600, Meta Loss: 0.005800459999591112\n",
      "Epoch 700, Meta Loss: 0.0051360665820539\n",
      "Epoch 800, Meta Loss: 0.00461765518411994\n",
      "Epoch 900, Meta Loss: 0.00430500041693449\n",
      "Epoch 1000, Meta Loss: 0.003940384369343519\n",
      "Epoch 1100, Meta Loss: 0.003631718922406435\n",
      "Epoch 1200, Meta Loss: 0.003393186954781413\n",
      "Epoch 1300, Meta Loss: 0.003159706946462393\n",
      "Epoch 1400, Meta Loss: 0.003005157457664609\n",
      "Model Evaluation Metrics:\n",
      "T: RÂ² Score = 0.9552, Percentage Error = 7.87%\n",
      "TR: RÂ² Score = 0.6601, Percentage Error = 22.64%\n",
      "m_Cu: RÂ² Score = 0.9973, Percentage Error = 2.85%\n",
      "m_mag: RÂ² Score = 0.9316, Percentage Error = 132527200.00%\n",
      "cos_phi: RÂ² Score = 0.9784, Percentage Error = 5.51%\n",
      "VM: RÂ² Score = 0.9754, Percentage Error = 10.99%\n",
      "Temp: RÂ² Score = 0.9945, Percentage Error = 0.77%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_data():\n",
    "    mot_a = pd.read_csv(\"Mot_A_rel_2.csv\")\n",
    "    mot_b = pd.read_csv(\"Mot_B_rel_1.csv\")\n",
    "    \n",
    "    X_a, Y_a = mot_a.iloc[:, :11].values, mot_a.iloc[:, 11:].values\n",
    "    X_b, Y_b = mot_b.iloc[:, :11].values, mot_b.iloc[:, 11:].values\n",
    "    \n",
    "    scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "    \n",
    "    X_a = scaler_x.fit_transform(X_a)\n",
    "    Y_a = scaler_y.fit_transform(Y_a)\n",
    "    X_b = scaler_x.transform(X_b)\n",
    "    Y_b = scaler_y.transform(Y_b)\n",
    "    \n",
    "    return {\n",
    "        \"train_x\": torch.tensor(X_a, dtype=torch.float32, device=device),\n",
    "        \"train_y\": torch.tensor(Y_a, dtype=torch.float32, device=device),\n",
    "        \"test_x\": torch.tensor(X_b, dtype=torch.float32, device=device),\n",
    "        \"test_y\": torch.tensor(Y_b, dtype=torch.float32, device=device),\n",
    "        \"scalers\": (scaler_x, scaler_y),\n",
    "        \"column_names\": mot_b.columns[11:].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self, input_size=11, output_size=7, hidden_size=128):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu2(self.bn2(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class MAML:\n",
    "    def __init__(self, model, meta_lr=0.001, inner_lr=0.01, inner_steps=1, device=device):\n",
    "        self.model = model.to(device)\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.loss_fn = nn.HuberLoss(delta=1.0)\n",
    "        self.device = device\n",
    "\n",
    "    def inner_update(self, X, y):\n",
    "        model_copy = MAMLModel().to(self.device)\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        optimizer = optim.Adam(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(self.inner_steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "    def meta_train(self, X_train, y_train, epochs=1500):\n",
    "        for epoch in range(epochs):\n",
    "            model_copy = self.inner_update(X_train, y_train)\n",
    "            loss = self.loss_fn(self.model(X_train), y_train)\n",
    "            \n",
    "            self.meta_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.meta_optimizer.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Meta Loss: {loss.item()}\")\n",
    "    \n",
    "    def fine_tune(self, X, y, steps=50):\n",
    "        model_copy = self.inner_update(X, y)\n",
    "        optimizer = optim.Adam(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "# Load data\n",
    "data = load_data()\n",
    "\n",
    "# Train MAML model on Motor A only\n",
    "model = MAMLModel()\n",
    "maml = MAML(model, device=device)\n",
    "\n",
    "maml.meta_train(data[\"train_x\"], data[\"train_y\"])  # Train on A\n",
    "\n",
    "# Fine-tune and evaluate on Motor B\n",
    "adapted_model = maml.fine_tune(data[\"test_x\"], data[\"test_y\"])\n",
    "\n",
    "y_pred = adapted_model(data[\"test_x\"]).detach().cpu().numpy()\n",
    "y_pred = data[\"scalers\"][1].inverse_transform(y_pred)\n",
    "y_test = data[\"scalers\"][1].inverse_transform(data[\"test_y\"].cpu().numpy())\n",
    "\n",
    "# Compute RÂ² and percentage error per column\n",
    "def evaluate_model(y_true, y_pred, column_names):\n",
    "    print(\"Model Evaluation Metrics:\")\n",
    "    for i, col in enumerate(column_names):\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        error = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / (y_true[:, i] + 1e-8))) * 100\n",
    "        print(f\"{col}: RÂ² Score = {r2:.4f}, Percentage Error = {error:.2f}%\")\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(y_test, y_pred, data['column_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61958e2-4004-46de-8a85-f748c772d2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
