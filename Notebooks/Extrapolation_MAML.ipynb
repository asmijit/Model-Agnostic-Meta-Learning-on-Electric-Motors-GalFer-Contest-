{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aea979ae-1df4-40d1-804a-4e3fbad45b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T13:07:13.263736Z",
     "iopub.status.busy": "2025-03-02T13:07:13.263374Z",
     "iopub.status.idle": "2025-03-02T13:07:13.322847Z",
     "shell.execute_reply": "2025-03-02T13:07:13.321835Z",
     "shell.execute_reply.started": "2025-03-02T13:07:13.263708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_alpha</th>\n",
       "      <th>h_c</th>\n",
       "      <th>r</th>\n",
       "      <th>w_t</th>\n",
       "      <th>l_t</th>\n",
       "      <th>w_o</th>\n",
       "      <th>dxIB</th>\n",
       "      <th>gamma</th>\n",
       "      <th>d_alpha_deg</th>\n",
       "      <th>hc_mm</th>\n",
       "      <th>w_o_mm</th>\n",
       "      <th>T</th>\n",
       "      <th>TR</th>\n",
       "      <th>m_Cu</th>\n",
       "      <th>m_mag</th>\n",
       "      <th>cos_phi</th>\n",
       "      <th>VM</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683555</td>\n",
       "      <td>0.652960</td>\n",
       "      <td>76.388367</td>\n",
       "      <td>4.876673</td>\n",
       "      <td>15.980657</td>\n",
       "      <td>0.252080</td>\n",
       "      <td>3.958199</td>\n",
       "      <td>47.979187</td>\n",
       "      <td>20.5065</td>\n",
       "      <td>13.333585</td>\n",
       "      <td>2.261068</td>\n",
       "      <td>322.034019</td>\n",
       "      <td>46.460214</td>\n",
       "      <td>3.496543</td>\n",
       "      <td>1.637520</td>\n",
       "      <td>0.600832</td>\n",
       "      <td>449.306137</td>\n",
       "      <td>161.493855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.839970</td>\n",
       "      <td>0.316877</td>\n",
       "      <td>66.528144</td>\n",
       "      <td>5.477306</td>\n",
       "      <td>21.227689</td>\n",
       "      <td>0.125430</td>\n",
       "      <td>-0.907076</td>\n",
       "      <td>38.605065</td>\n",
       "      <td>25.1991</td>\n",
       "      <td>2.806290</td>\n",
       "      <td>0.981157</td>\n",
       "      <td>319.186347</td>\n",
       "      <td>26.164250</td>\n",
       "      <td>3.422449</td>\n",
       "      <td>0.827423</td>\n",
       "      <td>0.460323</td>\n",
       "      <td>462.037013</td>\n",
       "      <td>163.451036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.430152</td>\n",
       "      <td>70.092293</td>\n",
       "      <td>5.902179</td>\n",
       "      <td>17.496837</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>2.421620</td>\n",
       "      <td>32.137510</td>\n",
       "      <td>22.4898</td>\n",
       "      <td>6.407833</td>\n",
       "      <td>1.638763</td>\n",
       "      <td>365.429403</td>\n",
       "      <td>18.859974</td>\n",
       "      <td>2.550712</td>\n",
       "      <td>1.383712</td>\n",
       "      <td>0.601910</td>\n",
       "      <td>376.266906</td>\n",
       "      <td>175.349022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.779947</td>\n",
       "      <td>0.588121</td>\n",
       "      <td>64.097056</td>\n",
       "      <td>3.983202</td>\n",
       "      <td>19.711625</td>\n",
       "      <td>0.325027</td>\n",
       "      <td>-2.712820</td>\n",
       "      <td>52.763373</td>\n",
       "      <td>23.3985</td>\n",
       "      <td>6.980233</td>\n",
       "      <td>2.450559</td>\n",
       "      <td>388.445550</td>\n",
       "      <td>43.657137</td>\n",
       "      <td>4.056152</td>\n",
       "      <td>1.559041</td>\n",
       "      <td>0.687662</td>\n",
       "      <td>362.526070</td>\n",
       "      <td>161.401494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671491</td>\n",
       "      <td>0.359245</td>\n",
       "      <td>61.665401</td>\n",
       "      <td>6.232320</td>\n",
       "      <td>21.751120</td>\n",
       "      <td>0.294789</td>\n",
       "      <td>1.796389</td>\n",
       "      <td>57.935310</td>\n",
       "      <td>20.1447</td>\n",
       "      <td>6.039823</td>\n",
       "      <td>2.139157</td>\n",
       "      <td>316.498500</td>\n",
       "      <td>43.529585</td>\n",
       "      <td>2.219354</td>\n",
       "      <td>0.930888</td>\n",
       "      <td>0.656835</td>\n",
       "      <td>265.543625</td>\n",
       "      <td>183.992897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>0.795862</td>\n",
       "      <td>0.389833</td>\n",
       "      <td>61.630750</td>\n",
       "      <td>4.726709</td>\n",
       "      <td>21.116910</td>\n",
       "      <td>0.391967</td>\n",
       "      <td>-0.356609</td>\n",
       "      <td>38.449031</td>\n",
       "      <td>23.8758</td>\n",
       "      <td>4.098186</td>\n",
       "      <td>2.842765</td>\n",
       "      <td>330.704739</td>\n",
       "      <td>52.500055</td>\n",
       "      <td>3.502802</td>\n",
       "      <td>0.961655</td>\n",
       "      <td>0.525826</td>\n",
       "      <td>364.070648</td>\n",
       "      <td>166.587400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>0.655776</td>\n",
       "      <td>0.557722</td>\n",
       "      <td>64.138009</td>\n",
       "      <td>5.569651</td>\n",
       "      <td>19.406519</td>\n",
       "      <td>0.286441</td>\n",
       "      <td>5.134185</td>\n",
       "      <td>41.751688</td>\n",
       "      <td>19.6734</td>\n",
       "      <td>10.213618</td>\n",
       "      <td>2.160973</td>\n",
       "      <td>258.916385</td>\n",
       "      <td>29.670157</td>\n",
       "      <td>2.627395</td>\n",
       "      <td>1.026045</td>\n",
       "      <td>0.541985</td>\n",
       "      <td>309.458343</td>\n",
       "      <td>176.949769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>0.824020</td>\n",
       "      <td>0.424761</td>\n",
       "      <td>70.060731</td>\n",
       "      <td>4.900752</td>\n",
       "      <td>17.915470</td>\n",
       "      <td>0.113394</td>\n",
       "      <td>0.268017</td>\n",
       "      <td>51.125728</td>\n",
       "      <td>24.7206</td>\n",
       "      <td>4.420288</td>\n",
       "      <td>0.933583</td>\n",
       "      <td>390.552354</td>\n",
       "      <td>52.783734</td>\n",
       "      <td>3.411842</td>\n",
       "      <td>1.220441</td>\n",
       "      <td>0.620082</td>\n",
       "      <td>445.986053</td>\n",
       "      <td>164.116517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>0.714139</td>\n",
       "      <td>0.334924</td>\n",
       "      <td>66.488205</td>\n",
       "      <td>3.919224</td>\n",
       "      <td>21.625679</td>\n",
       "      <td>0.187517</td>\n",
       "      <td>1.236156</td>\n",
       "      <td>58.018170</td>\n",
       "      <td>21.4242</td>\n",
       "      <td>5.356057</td>\n",
       "      <td>1.465975</td>\n",
       "      <td>360.733017</td>\n",
       "      <td>44.419942</td>\n",
       "      <td>5.033340</td>\n",
       "      <td>1.066646</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>321.930873</td>\n",
       "      <td>151.601869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>0.759187</td>\n",
       "      <td>0.695997</td>\n",
       "      <td>76.420941</td>\n",
       "      <td>5.769777</td>\n",
       "      <td>15.696193</td>\n",
       "      <td>0.361159</td>\n",
       "      <td>-3.897957</td>\n",
       "      <td>37.392224</td>\n",
       "      <td>22.7757</td>\n",
       "      <td>10.966983</td>\n",
       "      <td>3.240842</td>\n",
       "      <td>379.812048</td>\n",
       "      <td>42.086200</td>\n",
       "      <td>2.812695</td>\n",
       "      <td>2.333271</td>\n",
       "      <td>0.660957</td>\n",
       "      <td>450.984029</td>\n",
       "      <td>169.259025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       d_alpha       h_c          r       w_t        l_t       w_o      dxIB  \\\n",
       "0     0.683555  0.652960  76.388367  4.876673  15.980657  0.252080  3.958199   \n",
       "1     0.839970  0.316877  66.528144  5.477306  21.227689  0.125430 -0.907076   \n",
       "2     0.749656  0.430152  70.092293  5.902179  17.496837  0.198953  2.421620   \n",
       "3     0.779947  0.588121  64.097056  3.983202  19.711625  0.325027 -2.712820   \n",
       "4     0.671491  0.359245  61.665401  6.232320  21.751120  0.294789  1.796389   \n",
       "...        ...       ...        ...       ...        ...       ...       ...   \n",
       "4091  0.795862  0.389833  61.630750  4.726709  21.116910  0.391967 -0.356609   \n",
       "4092  0.655776  0.557722  64.138009  5.569651  19.406519  0.286441  5.134185   \n",
       "4093  0.824020  0.424761  70.060731  4.900752  17.915470  0.113394  0.268017   \n",
       "4094  0.714139  0.334924  66.488205  3.919224  21.625679  0.187517  1.236156   \n",
       "4095  0.759187  0.695997  76.420941  5.769777  15.696193  0.361159 -3.897957   \n",
       "\n",
       "          gamma  d_alpha_deg      hc_mm    w_o_mm           T         TR  \\\n",
       "0     47.979187      20.5065  13.333585  2.261068  322.034019  46.460214   \n",
       "1     38.605065      25.1991   2.806290  0.981157  319.186347  26.164250   \n",
       "2     32.137510      22.4898   6.407833  1.638763  365.429403  18.859974   \n",
       "3     52.763373      23.3985   6.980233  2.450559  388.445550  43.657137   \n",
       "4     57.935310      20.1447   6.039823  2.139157  316.498500  43.529585   \n",
       "...         ...          ...        ...       ...         ...        ...   \n",
       "4091  38.449031      23.8758   4.098186  2.842765  330.704739  52.500055   \n",
       "4092  41.751688      19.6734  10.213618  2.160973  258.916385  29.670157   \n",
       "4093  51.125728      24.7206   4.420288  0.933583  390.552354  52.783734   \n",
       "4094  58.018170      21.4242   5.356057  1.465975  360.733017  44.419942   \n",
       "4095  37.392224      22.7757  10.966983  3.240842  379.812048  42.086200   \n",
       "\n",
       "          m_Cu     m_mag   cos_phi          VM        Temp  \n",
       "0     3.496543  1.637520  0.600832  449.306137  161.493855  \n",
       "1     3.422449  0.827423  0.460323  462.037013  163.451036  \n",
       "2     2.550712  1.383712  0.601910  376.266906  175.349022  \n",
       "3     4.056152  1.559041  0.687662  362.526070  161.401494  \n",
       "4     2.219354  0.930888  0.656835  265.543625  183.992897  \n",
       "...        ...       ...       ...         ...         ...  \n",
       "4091  3.502802  0.961655  0.525826  364.070648  166.587400  \n",
       "4092  2.627395  1.026045  0.541985  309.458343  176.949769  \n",
       "4093  3.411842  1.220441  0.620082  445.986053  164.116517  \n",
       "4094  5.033340  1.066646  0.585498  321.930873  151.601869  \n",
       "4095  2.812695  2.333271  0.660957  450.984029  169.259025  \n",
       "\n",
       "[4096 rows x 18 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mot_a=pd.read_csv('/kaggle/input/motors/Mot_A_rel_2.csv')\n",
    "mot_b=pd.read_csv('/kaggle/input/motors/Mot_B_rel_1.csv')\n",
    "mot_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4aef7-c764-490d-b161-75a414d20f95",
   "metadata": {},
   "source": [
    " # INTRAPOLATION CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d378559-35fb-46e4-bb05-d84a29b1d8f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T12:47:07.134790Z",
     "iopub.status.busy": "2025-03-02T12:47:07.134434Z",
     "iopub.status.idle": "2025-03-02T12:47:10.637011Z",
     "shell.execute_reply": "2025-03-02T12:47:10.635958Z",
     "shell.execute_reply.started": "2025-03-02T12:47:07.134763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Meta Loss: 1.0210092067718506\n",
      "Epoch 100, Meta Loss: 0.10090705752372742\n",
      "Epoch 200, Meta Loss: 0.07084202021360397\n",
      "Epoch 300, Meta Loss: 0.05194752290844917\n",
      "Epoch 400, Meta Loss: 0.04164756089448929\n",
      "Epoch 500, Meta Loss: 0.03286929428577423\n",
      "Epoch 600, Meta Loss: 0.027026664465665817\n",
      "Epoch 700, Meta Loss: 0.02323191799223423\n",
      "Epoch 800, Meta Loss: 0.02055322378873825\n",
      "Epoch 900, Meta Loss: 0.01929798536002636\n",
      "Final MSE on 20% test set: 86.39111328125\n",
      "Final RMSE on 20% test set: 9.294681549072266\n",
      "Final MAE on 20% test set: 3.704435348510742\n",
      "Final RÂ² Score on 20% test set: 0.9760610411873577\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    mot_a = pd.read_csv(\"/kaggle/input/motors/Mot_A_rel_2.csv\")\n",
    "    mot_b = pd.read_csv(\"/kaggle/input/motors/Mot_B_rel_1.csv\")\n",
    "    \n",
    "    X_a, Y_a = mot_a.iloc[:, :11].values, mot_a.iloc[:, 11:].values\n",
    "    X_b, Y_b = mot_b.iloc[:, :11].values, mot_b.iloc[:, 11:].values\n",
    "    \n",
    "    X_combined = np.vstack([X_a, X_b])\n",
    "    Y_combined = np.vstack([Y_a, Y_b])\n",
    "    \n",
    "    scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "    X_combined = scaler_x.fit_transform(X_combined)\n",
    "    Y_combined = scaler_y.fit_transform(Y_combined)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_combined, Y_combined, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return (torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32)), \\\n",
    "           (torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.float32)), \\\n",
    "           scaler_x, scaler_y\n",
    "\n",
    "# Define MAML Model\n",
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self, input_size=11, output_size=7, hidden_size=128):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# MAML Meta-Learning Class\n",
    "class MAML:\n",
    "    def __init__(self, model, meta_lr=0.001, inner_lr=0.01, inner_steps=1, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    def inner_update(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = MAMLModel().to(self.device)\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(self.inner_steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)  # âœ… Ensure graph is retained\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "    def meta_train(self, X_train, y_train, epochs=1000):\n",
    "        X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model_copy = self.inner_update(X_train, y_train)\n",
    "            loss = self.loss_fn(self.model(X_train), y_train)  # âœ… Compute loss using meta-model\n",
    "            \n",
    "            self.meta_optimizer.zero_grad()\n",
    "            grads = torch.autograd.grad(loss, self.model.parameters(), create_graph=True, retain_graph=True)\n",
    "            \n",
    "            for param, grad in zip(self.model.parameters(), grads):\n",
    "                if grad is not None:\n",
    "                    param.grad = grad  # âœ… Correctly setting gradient\n",
    "            \n",
    "            self.meta_optimizer.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Meta Loss: {loss.item()}\")\n",
    "    \n",
    "    def fine_tune(self, X, y, steps=10):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = self.inner_update(X, y)\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "# Run MAML Training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "(X_train, y_train), (X_test, y_test), scaler_x, scaler_y = load_data()\n",
    "\n",
    "model = MAMLModel()\n",
    "maml = MAML(model, device=device)\n",
    "\n",
    "maml.meta_train(X_train, y_train)\n",
    "\n",
    "adapted_model = maml.fine_tune(X_test, y_test)\n",
    "\n",
    "y_pred = adapted_model(X_test.to(device)).detach().cpu().numpy()\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "y_test = scaler_y.inverse_transform(y_test.numpy())\n",
    "\n",
    "# Rigorous evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final MSE on 20% test set: {mse}\")\n",
    "print(f\"Final RMSE on 20% test set: {rmse}\")\n",
    "print(f\"Final MAE on 20% test set: {mae}\")\n",
    "print(f\"Final RÂ² Score on 20% test set: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9498e-1439-48f8-916a-b653ad23244a",
   "metadata": {},
   "source": [
    "## EXTRAPOLATION CODE BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65181710-030d-4291-855e-beca4b77ef22",
   "metadata": {},
   "source": [
    "# Training on 90% A & 20 % B\n",
    "# Test on only 80% B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eb8e178-c4b8-4a44-960b-a4d71a4f8b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:07.929359Z",
     "iopub.status.busy": "2025-03-02T16:08:07.929026Z",
     "iopub.status.idle": "2025-03-02T16:08:17.704926Z",
     "shell.execute_reply": "2025-03-02T16:08:17.704228Z",
     "shell.execute_reply.started": "2025-03-02T16:08:07.929331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Meta Loss: 6.084705829620361\n",
      "Epoch 100, Meta Loss: 0.27232009172439575\n",
      "Epoch 200, Meta Loss: 0.16080611944198608\n",
      "Epoch 300, Meta Loss: 0.11853639781475067\n",
      "Epoch 400, Meta Loss: 0.09423196315765381\n",
      "Epoch 500, Meta Loss: 0.0787685215473175\n",
      "Epoch 600, Meta Loss: 0.06691930443048477\n",
      "Epoch 700, Meta Loss: 0.05648048594594002\n",
      "Epoch 800, Meta Loss: 0.04813629388809204\n",
      "Epoch 900, Meta Loss: 0.041935428977012634\n",
      "Epoch 1000, Meta Loss: 0.037475451827049255\n",
      "Epoch 1100, Meta Loss: 0.033876195549964905\n",
      "Epoch 1200, Meta Loss: 0.03104274719953537\n",
      "Epoch 1300, Meta Loss: 0.02899795025587082\n",
      "Epoch 1400, Meta Loss: 0.026858679950237274\n",
      "Final MSE on simulated Motor C: 143.76060485839844\n",
      "Final RMSE on simulated Motor C: 11.990020751953125\n",
      "Final MAE on simulated Motor C: 4.6782073974609375\n",
      "Final RÂ² Score on simulated Motor C: 0.9471390221383352\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    mot_a = pd.read_csv(\"/kaggle/input/motors/Mot_A_rel_2.csv\")\n",
    "    mot_b = pd.read_csv(\"/kaggle/input/motors/Mot_B_rel_1.csv\")\n",
    "    \n",
    "    X_a, Y_a = mot_a.iloc[:, :11].values, mot_a.iloc[:, 11:].values\n",
    "    X_b, Y_b = mot_b.iloc[:, :11].values, mot_b.iloc[:, 11:].values\n",
    "    \n",
    "    scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "    \n",
    "    X_a = scaler_x.fit_transform(X_a)\n",
    "    Y_a = scaler_y.fit_transform(Y_a)\n",
    "    X_b = scaler_x.transform(X_b)\n",
    "    Y_b = scaler_y.transform(Y_b)\n",
    "    \n",
    "    X_train_a, X_test_a, Y_train_a, Y_test_a = train_test_split(X_a, Y_a, test_size=0.1, random_state=42)\n",
    "    X_train_b, X_test_b, Y_train_b, Y_test_b = train_test_split(X_b, Y_b, test_size=0.8, random_state=42)  # âœ… Hold out 50% of Motor B as unseen Motor C\n",
    "    \n",
    "    return {\n",
    "        \"motor_a\": (torch.tensor(X_train_a, dtype=torch.float32), torch.tensor(Y_train_a, dtype=torch.float32)),\n",
    "        \"motor_b\": (torch.tensor(X_train_b, dtype=torch.float32), torch.tensor(Y_train_b, dtype=torch.float32)),\n",
    "        \"test\": (torch.tensor(X_test_b, dtype=torch.float32), torch.tensor(Y_test_b, dtype=torch.float32)),  # âœ… Treat as Motor C (unseen)\n",
    "        \"scalers\": (scaler_x, scaler_y)\n",
    "    }\n",
    "\n",
    "# Define MAML Model\n",
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self, input_size=11, output_size=7, hidden_size=128):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# MAML Meta-Learning Class\n",
    "class MAML:\n",
    "    def __init__(self, model, meta_lr=0.001, inner_lr=0.01, inner_steps=1, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    def inner_update(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = MAMLModel().to(self.device)\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(self.inner_steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)  # âœ… Ensure graph is retained\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "    def meta_train(self, tasks, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            meta_loss = 0\n",
    "            for task_name, (X_train, y_train) in tasks.items():\n",
    "                model_copy = self.inner_update(X_train, y_train)\n",
    "                loss = self.loss_fn(self.model(X_train.to(self.device)), y_train.to(self.device))\n",
    "                meta_loss += loss\n",
    "            \n",
    "            meta_loss /= len(tasks)  # âœ… Aggregate loss across tasks\n",
    "            \n",
    "            self.meta_optimizer.zero_grad()\n",
    "            grads = torch.autograd.grad(meta_loss, self.model.parameters(), create_graph=True, retain_graph=True)\n",
    "            \n",
    "            for param, grad in zip(self.model.parameters(), grads):\n",
    "                if grad is not None:\n",
    "                    param.grad = grad  # âœ… Correctly setting gradient\n",
    "            \n",
    "            self.meta_optimizer.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Meta Loss: {meta_loss.item()}\")\n",
    "    \n",
    "    def fine_tune(self, X, y, steps=10):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = self.inner_update(X, y)\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "# Run MAML Training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data = load_data()\n",
    "\n",
    "model = MAMLModel()\n",
    "maml = MAML(model, device=device)\n",
    "\n",
    "maml.meta_train({\"motor_a\": data[\"motor_a\"], \"motor_b\": data[\"motor_b\"]})  # âœ… Train on Motor A & B\n",
    "\n",
    "adapted_model = maml.fine_tune(*data[\"test\"])  # âœ… Fine-tune on unseen Motor C (held-out part of Motor B)\n",
    "\n",
    "y_pred = adapted_model(data[\"test\"][0].to(device)).detach().cpu().numpy()\n",
    "y_pred = data[\"scalers\"][1].inverse_transform(y_pred)  # âœ… Use Motor C's scaler\n",
    "y_test = data[\"scalers\"][1].inverse_transform(data[\"test\"][1].numpy())\n",
    "\n",
    "# Rigorous evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final MSE on simulated Motor C: {mse}\")\n",
    "print(f\"Final RMSE on simulated Motor C: {rmse}\")\n",
    "print(f\"Final MAE on simulated Motor C: {mae}\")\n",
    "print(f\"Final RÂ² Score on simulated Motor C: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44760a5e-c93f-4cb2-979c-531833d9464c",
   "metadata": {},
   "source": [
    "# Training on  80% A and  80% B. Testing on rest combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06229f7e-4428-4f01-984e-3dc972a92b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:07:39.539639Z",
     "iopub.status.busy": "2025-03-02T16:07:39.539290Z",
     "iopub.status.idle": "2025-03-02T16:07:55.956758Z",
     "shell.execute_reply": "2025-03-02T16:07:55.955917Z",
     "shell.execute_reply.started": "2025-03-02T16:07:39.539613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Meta Loss: 1.441690444946289\n",
      "Epoch 100, Meta Loss: 0.7259581089019775\n",
      "Epoch 200, Meta Loss: 0.5267054438591003\n",
      "Epoch 300, Meta Loss: 0.20459023118019104\n",
      "Epoch 400, Meta Loss: 0.08324484527111053\n",
      "Epoch 500, Meta Loss: 0.049188368022441864\n",
      "Epoch 600, Meta Loss: 0.038733936846256256\n",
      "Epoch 700, Meta Loss: 0.03296838328242302\n",
      "Epoch 800, Meta Loss: 0.029573433101177216\n",
      "Epoch 900, Meta Loss: 0.026726502925157547\n",
      "Epoch 1000, Meta Loss: 0.02435028925538063\n",
      "Epoch 1100, Meta Loss: 0.023025885224342346\n",
      "Epoch 1200, Meta Loss: 0.0218255203217268\n",
      "Epoch 1300, Meta Loss: 0.020890012383461\n",
      "Epoch 1400, Meta Loss: 0.020596351474523544\n",
      "Final MSE on combined testMotor B: 292.5282897949219\n",
      "Final RMSE on Motor B: 17.103458404541016\n",
      "Final MAE on Motor B: 8.247387886047363\n",
      "Final RÂ² Score on Motor B: 0.8709897311989698\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    mot_a = pd.read_csv(\"/kaggle/input/motors/Mot_A_rel_2.csv\")\n",
    "    mot_b = pd.read_csv(\"/kaggle/input/motors/Mot_B_rel_1.csv\")\n",
    "    \n",
    "    X_a, Y_a = mot_a.iloc[:, :11].values, mot_a.iloc[:, 11:].values\n",
    "    X_b, Y_b = mot_b.iloc[:, :11].values, mot_b.iloc[:, 11:].values\n",
    "    \n",
    "    scaler_x, scaler_y = StandardScaler(), StandardScaler()\n",
    "    \n",
    "    X_a = scaler_x.fit_transform(X_a)\n",
    "    Y_a = scaler_y.fit_transform(Y_a)\n",
    "    X_b = scaler_x.transform(X_b)\n",
    "    Y_b = scaler_y.transform(Y_b)\n",
    "    \n",
    "    X_train_a, X_test_a, Y_train_a, Y_test_a = train_test_split(X_a, Y_a, test_size=0.2, random_state=42)\n",
    "    X_train_b, X_test_b, Y_train_b, Y_test_b = train_test_split(X_b, Y_b, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return {\n",
    "        \"motor_a\": (torch.tensor(X_train_a, dtype=torch.float32), torch.tensor(Y_train_a, dtype=torch.float32)),\n",
    "        \"motor_b\": (torch.tensor(X_train_b, dtype=torch.float32), torch.tensor(Y_train_b, dtype=torch.float32)),\n",
    "        \"test\": (torch.tensor(np.vstack([X_test_a, X_test_b]), dtype=torch.float32), \n",
    "                  torch.tensor(np.vstack([Y_test_a, Y_test_b]), dtype=torch.float32)),  # âœ… Test on combined 20% of A & B\n",
    "        \"scalers\": (scaler_x, scaler_y)\n",
    "    }\n",
    "\n",
    "# Define MAML Model\n",
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self, input_size=11, output_size=7, hidden_size=128):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu2(self.bn2(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# MAML Meta-Learning Class\n",
    "class MAML:\n",
    "    def __init__(self, model, meta_lr=0.001, inner_lr=0.01, inner_steps=1, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=meta_lr)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.loss_fn = nn.HuberLoss(delta=1.0)\n",
    "        self.device = device\n",
    "\n",
    "    def inner_update(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = MAMLModel().to(self.device)\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        optimizer = optim.Adam(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(self.inner_steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)  # âœ… Ensure graph is retained\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "    def meta_train(self, tasks, epochs=1500):\n",
    "        for epoch in range(epochs):\n",
    "            meta_loss = 0\n",
    "            for task_name, (X_train, y_train) in tasks.items():\n",
    "                model_copy = self.inner_update(X_train, y_train)\n",
    "                loss = self.loss_fn(self.model(X_train.to(self.device)), y_train.to(self.device))\n",
    "                meta_loss += loss\n",
    "            \n",
    "            meta_loss /= len(tasks)  # âœ… Aggregate loss across tasks\n",
    "            \n",
    "            self.meta_optimizer.zero_grad()\n",
    "            grads = torch.autograd.grad(meta_loss, self.model.parameters(), create_graph=True, retain_graph=True)\n",
    "            \n",
    "            for param, grad in zip(self.model.parameters(), grads):\n",
    "                if grad is not None:\n",
    "                    param.grad = grad  # âœ… Correctly setting gradient\n",
    "            \n",
    "            self.meta_optimizer.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Meta Loss: {meta_loss.item()}\")\n",
    "    \n",
    "    def fine_tune(self, X, y, steps=100):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        model_copy = self.inner_update(X, y)\n",
    "        optimizer = optim.Adam(model_copy.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            loss = self.loss_fn(model_copy(X), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model_copy\n",
    "\n",
    "# Run MAML Training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data = load_data()\n",
    "\n",
    "model = MAMLModel()\n",
    "maml = MAML(model, device=device)\n",
    "\n",
    "maml.meta_train({\"motor_a\": data[\"motor_a\"], \"motor_b\": data[\"motor_b\"]})  # âœ… Train only on Motor A\n",
    "\n",
    "adapted_model = maml.fine_tune(*data[\"test\"])  # âœ… Fine-tune on full Motor B\n",
    "\n",
    "y_pred = adapted_model(data[\"test\"][0].to(device)).detach().cpu().numpy()\n",
    "y_pred = data[\"scalers\"][1].inverse_transform(y_pred)  # âœ… Use Motor B's scaler\n",
    "y_test = data[\"scalers\"][1].inverse_transform(data[\"test\"][1].numpy())\n",
    "\n",
    "# Rigorous evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final MSE on combined testMotor B: {mse}\")\n",
    "print(f\"Final RMSE on Motor B: {rmse}\")\n",
    "print(f\"Final MAE on Motor B: {mae}\")\n",
    "print(f\"Final RÂ² Score on Motor B: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf9a52-3dce-4931-b23b-b9da2d88d1e3",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6772281,
     "sourceId": 10897411,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
